# 0. Prompt 
Use Stitch for loading All of Recurly

#1. Resolve Ambiguity
* What tables are we currently ETL'ing in via Stitch?
    * Look in snowflake

* Are the set of tables between the Kettle and SnowflakeLoader jobs the same?
    * Yes

* What were the additionally requested fields from finance after our first iteration?
    * Doesn't matter, let's just get it all

* Is it ok to just get all the fields in all the tables?
    * Yes, it'll satisfy Finance too

* The default it to get one year of historical data. Do we want more or is that ok?
    * No, let's just get everything from when we started using recurly

* The default is to replicate every half hour. Is that ok?
    * No, too frequent. Let's just do 2x a day

#2. Run through an example
Imagine we need all the tables since we started. 

#3. Brute Force
Get all the history from May 14 2013 on every 12 hours for all the tables

#4. Time and Space Analysis
Time: O(n) where n is the number of records
Space: O(n) where n in the number of records

#5. Walk through it. Does it make sense?
It does so long as the number of records is sufficiently small enough to not piss of Dan's superiors

#6. Code it up
No coding necessary. Integration currently running

#7. Test it
Need Dan to verify it vs the data snowflake loader or kettle

---
#8. What did I learn?
* Stitch is way easier than writing our own Kettle jobs or using an API

#9. Feedback?
Waiting



