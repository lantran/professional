# 0. Prompt
Is the new schema that the media library worker writes to adequate?

* Estimate write traffic by sync'ing with ME team
* Write a script (or docker container w/ script) to simulate write patterns

For example, up-sert video-count and bytes-stored.

Bonus points is to also simulate usage-mini writes

# 1. Resolve ambiguity
* Who will be writing to the Usage DB?
    1. The aggregation worker
    2. The media library worker
    
### Aggregation Worker(Optional)    
* What will the aggregation worker be writing?
    1. bytes_streamed from CDN logs
    2. plays and ad_impressions from ping data

* What are the write patterns of the aggregation worker?
    1. Inserts per site_ids for bytes_streamed from cdn data to the cdn_batch table
    2. Inserts per analytics_ids for plays ad_impression from ping data to the ping_batch table
    3. Compaction jobs from batch -> daily for cdn_batch
    4. Compaction jobs from batch -> daily for ping_batch
    5. Refreshing materialized views after each insert to cdn_batch and ping_batch

* Are compaction jobs that turn batch into daily in the scope of this?
    * If it's easy do it, if it's not don't
    * Note: Optional within the optional. The simulation of the aggregation worker is not strictly within scope for this ticket
    
* What's the relationship between site_ids and analytics_ids?
    * There is a one to one mapping site_id to analytics_id. *Usage service does not have the mapping*
    
* Who can I talk to about realistic throughput of the aggregation worker?
    
    _Steve Whalen says_
    * There are two jobs, one for fastly and one for edgecast (CDN)
    * These are grouped into the aggregation worker
    * If you take a look at the run_start_time table, you can get an idea about the start and end times of those jobs ~6-8 minutes
    * Queries to help get an idea of runtime, size, and frequency of the aggregation worker runs:
    ```sql
    select * from usage_mini_runs where job = 2 and pdate = '2017-11-29' order by run_start_time desc;
    
    select batch_id, count(*) from ping_batch_aggregates where pdate = '2017-11-29' group by batch_id order by batch_id;
    
    select batch_id, count(*) from cdn_batch_aggregates where pdate = '2017-11-29' group by batch_id order by batch_id;
    ```
    * Could get away with a data generator
    * Could use real data for a given day __Preferred__

### Media Library Worker(Mandatory)
* What will the media library worker be writing?
    1. bytes_stored and media_count

* What are the write patterns of the media library worker?
    1. Upserts per date and site_id for media count and bytes stored to the media storage table
        * NOTE: Sync with Sahil on this. Right now 3-4x a day but eventually streaming

* Is the media library worker responsible for max_bytes_stored and max_video_count? If not then who?
    * Usage service is responsible for this. It will be done on upsert with the max of current and the newest state


* Who can I talk to about realistic throughput of the media library worker?
    
    _Tom Boshoven says_
    * This data is currently written to BOTR, correct? Is it available elsewhere?
        * Yes, it is in BOTR
        * There are replicas of BOTR and a backup (would have to ask devops)
        * Copy data out of BOTR
            * There are attributes in there which are confidential
    
    * Other than a per date, per site_id, media_count event, is there anything else that will be written to Usage?
        * Is this one event or two separate ones?
            * This appears to be two separate events. Transactional safety of RDBMS will be useful here
            * This is because this might come from two different sources in the future
        
    * What is the frequency with which this data is written?
        * Every 30 seconds, updating about 2000 users
        * Double checking with Bram about this figure
    
    * Are there are copies of real data in S3?
        * No
    
    * How would I go about generating data of the appropriate cardinality to simulate realistic conditions?
    
    * What would the ratio of bytes_stored events to media_count events?

* Will we be backfilling historical data into the new usage service?
    * Yes, but that is outside the scope of this ticket
    
* How long will the load tests be?
Long enough to simulate 1 batch where all writes strive to recreate the worst case scenario

# 2. Walk through an example
### Media Library Worker
* A SQL script that upserts Y rows per N minutes mimicking the number of events to the media_storage table
### Aggregation Worker
* A SQL script that inserts 2X rows per N minutes to the cdn table
* A SQL script that inserts X rows per N minutes to the pings table

#### Note:
* To best simulate write load, the three scripts would be run concurrently and in their own processes
* To best simulate the network bottlenecks, it'd be best if the SQL being executed did so over a networked connection (like being in it's own docker container)

# 3. Brute Force Approach
* [X] Create a directory of SQL files
* [] Create a SQL file for each one of the processes you want to simulate
    - Media Library worker currently runs every 2 minutes on a cron, updates up to 68 entries, and takes ~30 s
    - Aggregation worker runs every 30 minutes, takes ~8 minutes to write, inserts TODO rows
* [] Create a python main that reads in the files and replaces values
    - site_id -> create up to 999 `mlw_###` prepended site_ids
    - utc_date -> keep the same for up to 24L(L per hour * 24 hours a day) iterations of the loop
    - analytics_id -> create up to 99999 `aw_#######` prepended analytics_ids 
    - each SQL file get its own process
    - We make the distinction between batch writers and stream writers
* [] Write those strings to the database using cursor.execute of from psycopg2 lib

Interface
python simulate_writers.py --services=aggregations,media_library --runs_per_hour=L

If you wanted some batch and some streaming jobs, you'd run the following command

ex.
python simulate_writers.py --services=aggregations,media_library --runs_per_hour=L (only applies to batched)

# 4. Time and Space Analysis
* Time: 0(Y + 3X) as you need to iterate all lines read in to substitute values
* Space: O(Y + 3X) as all files must be held in memory simultaneously to simulate concurrent writes

# 5. Walk through your solution. Does it make sense?
Yes - redesigned configuration to be a file instead of command line arguments

# 6. Code it up
Done

# 7. Test it
Yup

---
#8. What did I learn?
orchestrating multiple processes in python is simple

#9. Feedback
* [x] Simple changes
* [x] Change __ to _
* [x] Convert to os.path.join
* [x] Data store connection configurable
* [] Document the unexpected exit case scenario
* [] Add a queue to pass generated site_ids around