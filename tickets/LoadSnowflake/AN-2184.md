# 0. Prompt
From JIRA:
```
As a product manager, I want to see what customers are using the API within Snowflake/Looker and actively monitor their usage daily.
S3-production

s3://aws-elb-dev
s3://aws-elb-stg
s3://aws-elb-prd
Load into Snowflake
LOGS_DEV.public.API_GATEWAY
LOGS_STG.public.API_GATEWAY
LOGS_PRD.public.API_GATEWAY
Steps
1. Create databases
2. Create tables (relational or unstructured -> Use best judgement and share with team)
3. Create Stage for loading
4. Create job to upload
5. Create monitors for job
```

# 1. Resolve Ambiguity
1. What are the exact s3 paths?
elk-log-gateway-dev-elk-01
elk-log-gateway-stg-elk-01
elk-log-gateway-prd-elk-01

2. What is the shape of the data?
It is lines of valid json 

3. How are the log files broken up?
Seems pretty random

4. Are they easily queryable as is?


5. What fields are available?
input_type', 
'offset', 
'REQUEST_URI', 
'service', 
'@timestamp', 
'source', 
'container_id', 
'levelname', 
'message', 
'beat', 
'@version', 
'REMOTE_ADDR', 
'SOURCE', 
'timestamp', 
'QUERY_STRING', 
'asctime', 
'stage', 
'stream', 
'tags', 
'pod', 
'request_id', 
'REQUEST_METHOD', 
'type', 
'fields'

We need to load these in and join the query logs where query_logs.tracekey = gateway_logs.request_id
Since there will be multiple, we'll need to do a coalesce on the SOURCE to figure out dashboard|application|unknown
Healthchecks have SOURCE unknown FYI

6. How can we do this more than once a day? (Bonus)

7. How can we ensure we don't run into the same issues we did the first time with reporting query logs?

# 2. Create an example
Example one line of input:
```
{   '@timestamp': '2017-11-13T23:27:08.312Z',
    '@version': '1',
    'QUERY_STRING': '',
    'REMOTE_ADDR': '100.107.224.0',
    'REQUEST_METHOD': 'GET',
    'REQUEST_URI': '/internal/healthcheck/',
    'SOURCE': 'unknown',
    'asctime': '2017-11-13 23:27:08,312',
    'beat': {   'hostname': 'filebeat-tfpw6',
                'name': 'filebeat-tfpw6',
                'version': '5.2.2'},
    'container_id': 'a793f0125aef7d4865f786578c37e74316b94cdc9cd40631821b3a2acfa29e5f',
    'fields': {'application': 'docker'},
    'input_type': 'log',
    'levelname': 'INFO',
    'message': None,
    'offset': 824237,
    'pod': 'api-gateway-1484437524-rwgh9',
    'request_id': 'GqRgaDQAT_WF0IztVzwqmg',
    'service': 'api-gateway',
    'source': '/var/log/containers/api-gateway-1484437524-rwgh9_dev_api-gateway-a793f0125aef7d4865f786578c37e74316b94cdc9cd40631821b3a2acfa29e5f.log',
    'stage': 'dev',
    'stream': 'stderr',
    'tags': ['grokked', 'KUBERNETES', 'k8s', 'docker', 'origin_dated'],
    'timestamp': '2017-11-13T23:27:08.312758559Z',
    'type': 'kubernetes-docker'}
```


# 3. Brute Force a Solutions

0. Create the table in snowflake 
1. Download all the files from {bucket} in s3
2. Load them into memory
3. Transform it
4. Load into snowflake
```
for k in keys:
  lowercase everything. strip any @ signs
  inspect the value type.
```
This assumes the schema is mostly static as the table would have been created with a key-to-column name mapping 1-to-1.


# 4. Time and Space Analysis
Time: O(dl) where d is number of days and l is number of log files per day

# 5. Walk through the solution. Does it make sense? Can it be done more optimally?
###### 1. What should the interface to this functionality be?
start date, end_date, force-reload

##### 2. What should happen if there's a new/deleted key in the log object?
should it adapt on the fly or do we want to know there's a problem?

##### 3. What will be responsible for performing the join to fill in the SOURCE in the query logs?

##### 4. What would happen if there's a request_id that doesn't match up to a query log item tracekey?

##### 5. 

# 6. Code it up

# 7. Test it
